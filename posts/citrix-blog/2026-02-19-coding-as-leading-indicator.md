---
title: "What will knowledge work be in 18 months? Look at what AI is doing to coding right now."
date: "2026-02-19"
authority_level: 5
file_type: citrix-blog-post
tags: [knowledge-work, ai-coding, five-levels, dark-knowledge-factory, human-ai-collaboration, personal-ai-knowledge-systems, ai-verification]
related_frameworks: [five-levels-of-ai-in-knowledge-work, 7-stage-roadmap]
original_url: "https://www.citrix.com/blogs/2026/02/19/what-will-knowledge-work-be-in-18-months-look-at-what-ai-is-doing-to-coding-right-now"
staleness_threshold: stable
---

# What will knowledge work be in 18 months? Look at what AI is doing to coding right now.

[Original post](https://www.citrix.com/blogs/2026/02/19/what-will-knowledge-work-be-in-18-months-look-at-what-ai-is-doing-to-coding-right-now)

*Brian Madden—February 19, 2026*

There's a lot of buzz now that [something big is happening](https://www.nytimes.com/2026/02/18/opinion/ai-software.html?unlocked_article_code=1.NFA.x0W4.ZtHyUSoYsJ-L&smid=url-share) in software engineering thanks to the latest batch of AI coding models. Most knowledge workers think this is just a "coding thing" which doesn't apply to them. They're wrong.

Dan Shapiro, Glowforge CEO and Wharton Research Fellow, recently published a [five-level framework](https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/) which maps the level of AI assistance for coding from humans using it for simple searches all the way to a "dark factory", where AI is essentially just a black box that turns specs into software.

I want to walk through those five levels, because I think this pattern also applies to knowledge work, and we knowledge workers are only a couple of months behind coders in this regard.

## Shapiro's five levels of AI use in coding

(I've condensed but mostly used [his words](https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/) here)

* **Level 0: AI is spicy autocomplete.** You are doing manual coding and not a character hits the disk without your approval. You might use AI as a search super search engine, or occasionally accept an suggestion, but the code is unmistakably yours.

* **Level 1: AI is a coding intern.** You offload discrete tasks to AI. "Write a unit test." "Add a docstring." You're seeing speedups, but you're still moving at the rate you type.

* **Level 2: AI is a junior developer.** You a are "pair programmer" with the AI and now have a junior buddy to hand off all your boring stuff to. You're in a flow state and more productive than you've ever been. Shapiro says 90% of "AI-native" developers are here, and the danger is from level 2, and every level after it, the coder feels like they're done. They've maxed out. But they haven't.

* **Level 3: AI is a developer.** You're not the developer anymore—that's your AI's job. You're a manager. You're the human in the loop. Your coding agent is always running in multiple tabs and you spend your days reviewing code and changes. For many people, this feels like things got *worse*. Almost everyone tops out here.

* **Level 4: AI is an engineering Team.** Now you're not even a developer manager, you're a product manager. You write specs, argue with the AI about specs, craft skills plan schedules, then leave for 12 hours and check if the tests pass. (Shapiro says he's here.)

* **Level 5: AI is a dark software factory.** You are the engineering manager who sets the goals of the system in plain English. The AI defines implementation, writes code, tests, fixes bugs, and ships. It's not really a software process anymore. It's a black box that turns specs into software.

## AI & coding future thinking also applies to AI & knowledge work

[Nate B. Jones](https://www.natebjones.com) covers the AI-and-software engineering beat better than almost anyone. His [YouTube videos](https://www.youtube.com/@NateBJones) are "required watching" for me. I realized recently that everything he says about how AI is impacting software engineering also applies to AI impacting knowledge workers. For example, some quotes of his from recent videos:

> "The bottleneck has shifted. You are now the manager of however many agents you can keep track of productively. Your productive capacity is limited now only by your attention span and your ability to scope tasks well."

> "These are supervision problems, not capability problems. And the solution isn't to do the work yourself. It's to get better at your management skills."

In fact if you do some simple [Mad Libs style](https://en.wikipedia.org/wiki/Mad_Libs) find-and-replace, Nate's also a pretty good "future of work" strategist! Just swap out:

* code → deliverables / work product / output
* engineer → knowledge worker
* technical leader → business leader
* implementation → producing deliverables
* system → outcome
* tests → success criteria
* codebase → work stream
* syntax → formatting

Let's try that on some more quotes from his recent videos:

> "It's less time writing ~~code~~ **deliverables**. It's much more time defining what you want. It's much more time evaluating whether you got there."

> "Most ~~engineers~~ **knowledge workers** have spent years developing their intuitions around ~~implementation~~ **producing deliverables** and those are now not super useful. The new skill is describing the ~~system~~ **outcome** precisely enough that AI can build it, and then writing ~~tests~~ **success criteria** that capture what you actually need, and reviewing AI-generated ~~code~~ **output** for subtle conceptual errors rather than simple ~~syntax~~ **formatting** mistakes."

> "If you are not thinking through what you want done, the speed can lead you to very quickly build a giant pile of ~~code~~ **work product** that is not very useful. That is a superpower that everyone has been handed for better or worse and we are about to see who is actually able to think well."

> "We need to think as ~~technical~~ **business** leaders about where ~~engineers~~ **knowledge workers** should stand in relation to the ~~code~~ **AI-generated output** based on the risk profile of that ~~codebase~~ **work stream** itself."

That last one is illustrates the power of this perfectly because that's a concept that applies to software engineering but I never would have thought about in the context of knowledge work, but it 100% applies there as well. Which strategic deliverables require human review and which can you trust to the Dark Knowledge Factory?

## The five levels of AI use knowledge work

Now let's take Shapiro's five levels of AI use in coding and translate them to knowledge work. (Some of these loosely map to my own [7-stage roadmap for human-AI collaboration in the workplace](https://www.citrix.com/blogs/2025/06/24/the-7-stage-roadmap-for-human-ai-collaboration-in-the-workplace/) from six months ago, though Shapiro's levels address what the human-AI relationship is like, whereas I focused on the mechanics of the collaboration.)

Putting Shapiro's coding levels through our Mad Libs code-to-knowledge work translator:

* **Level 0: AI is a spicy search engine.** You are doing the knowledge work and not a word hits the page without your approval. You might use AI as a super search engine, or occasionally accept a suggested sentence, but the deliverable is unmistakably yours. This is most enterprise knowledge workers today.

* **Level 1: AI is a research intern.** You offload discrete tasks to AI. "Summarize this document." "Draft a response to this email." You're seeing speedups, but you're still moving at the rate you type. You're still the one producing the deliverable. This is most people's experience with Office Copilot.

* **Level 2: AI is a junior analyst.** You are "pair working" with AI and now have a junior buddy to hand off all your boring stuff to. You're in a flow state and more productive than you've ever been. This is using persistent AI collaboration spaces, like Google NotebookLM, Claude Projects, or Copilot Notebooks. Like their coding counterparts, from level 2 and every level after it, knowledge workers feel like they're done and they've maxed out. But they haven't.

* **Level 3: AI is an analyst.** You're not the one producing work anymore. (That's your AI's job.) You're the manager. You're the human in the loop. Your AI is always running and you spend your days reviewing and editing everything it generates. Strategy decks, market analyses, competitive intelligence, communications. Your life is tracked changes. For many people, this feels like things got *worse*. Almost everyone tops out here. This is where workers using a [personal AI knowledge system / "second brain"](https://www.linkedin.com/pulse/i-built-second-brain-using-ai-its-changed-way-work-future-madden-0tote/) are. This is where I am.

* **Level 4: AI is a strategy team.** Now you're not even a manager, you're a director. You don't write deliverables or even review them line by line. You write specs for deliverables. You define what a good competitive analysis looks like, what the acceptance criteria are, what scenarios it needs to handle. You craft the prompts, the system instructions, and the evaluation rubrics. Then you walk away and check if the output passes your scenarios.

* **Level 5: AI is a dark knowledge factory.** You are the executive who sets the goals of the organization in plain English. The AI defines approach, produces deliverables, evaluates quality, iterates, and ships. It's not really a work process anymore. It's a black box that turns business intent into business outcomes. A handful of people running what used to be an entire analyst function. The verification framework is the intellectual property, not the reports themselves.

## But how do you know the AI's work is any good?

I feel like I can follow along the analogy until Level 3, but Levels 4 and 5 seem weird to me and I don't know exactly how they would apply to knowledge work. (Heh, funny I'm personally at Level 3 and as Shapiro wrote, people after Level 2 think that whatever level they're at is the top.)

The hardest question at Level 4 and 5 is the same whether you're writing code or strategy memos: how do you verify the output without a human reviewing every piece?

In code, the answer turned out to be [end-to-end behavioral tests stored separately from the codebase](https://simonwillison.net/2026/Feb/7/software-factory/) (so the AI can't cheat). For knowledge work, I think it maps to something like:

* Rubrics as holdout sets. You define what a good strategy recommendation looks like. More than, "Is this well-written?" But instead, "Does this account for the competitor's likely response? Does this identify second-order effects? Would the CFO approve this?" These rubrics live outside the generation process, just like code tests live outside the codebase.
* Adversarial review agents. One AI generates the analysis. A different AI prompted to be a skeptical board member, a hostile competitor, or a regulatory lawyer tries to break it. The verification loop isn't human review. It's AI verifying AI against criteria that humans defined.

## AI's impact to software development in the enterprise was just the beginning. Get ready knowledge work!

Everyone's talking about this as a story about software engineering. "Look what AI's doing to software engineering!" "Look what AI's doing to coders!" "Hey coders, get on board or lose your livelihood."

But [coding was the beachhead, not the destination](https://www.citrix.com/blogs/2026/02/11/workers-second-brains-break-every-assumption-about-how-we-secure-knowledge-work/). Software was first because code has a built-in verification layers, languages have specific syntax, and there are billions of pages on the internet about how to write good code.

Knowledge work is next, and the timeline will be compressed. (We have better AI now and lots of lessons from the software world.) If frontier coding teams are at Level 4-5 today while frontier knowledge workers are at Level 1-2, a pretty good way to know what knowledge work looks like in 18 months is to look at what coders are doing right now.

Remember the bottleneck keeps moving. At Level 1 it's "how fast can you produce work?" At Level 4 it's "how precisely can you specify what should exist?" By Level 5 it's "how rigorously can you verify that it does?" That Level 5 one is a governance problem which nobody has a playbook for yet. Who owns the specs? Who defines the rubrics? Who's making sure the Dark Knowledge Factory isn't producing hallucinated strategy recommendations that look right but fall apart under scrutiny?

The career progression from "doing" to "directing" used to take 20 years. AI is compressing that to months. Most enterprises don't have [the governance infrastructure](https://www.citrix.com/blogs/2025/11/13/everyone-wants-to-provide-your-ai-nobody-wants-to-help-you-manage-it/) for any of this, and it's coming whether they're ready or not.
