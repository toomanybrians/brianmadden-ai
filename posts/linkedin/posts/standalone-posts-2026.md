---
title: "LinkedIn posts ‚Äî 2026"
date: "2026-02-27"
authority_level: 5
file_type: linkedin-posts-collection
tags: ["social-media", "short-form", "linkedin"]
staleness_threshold: weeks
description: "Collection of short-form LinkedIn feed posts from 2026. Updated as new posts are published."
---

This is a list of LinkedIn posts (actual "posts", not "articles") that I, Brian Madden, VP Technology Officer & Futurist at Citrix have posted to LinkedIn. This list is only "standalone" posts, e.g. ones that are not reposts and that do not link to blog posts, speeches, etc.

---

https://www.linkedin.com/posts/bmadden_salesforce-just-invented-a-metric-to-measure-share-7433078775040569344

Salesforce just invented a metric to measure the value that AI provides that might accidentally kill half its own product's value.

The "Agentic Work Unit" measures tasks completed by AI agents (rather than tokens), so generating documents, booking reservations, closing deals... The idea is to show customers the value they're getting per token spent.

But Salesforce puts a price tag on every task, customers are going to pretty quickly start scrutinizing which tasks are actually worth paying for.

My guess is the "fluff" of human-based knowledge work: status reports, TPS reports, weekly summaries, etc will be the first to go... basically anything summarizing mountains of work assembled for humans. They're vestiges of a world where the only way to move information between people was to package it into documents and meetings.

But AI doesn't need any of that. An AI agent doesn't need a status report to know what's happening‚Äîit can just look. It doesn't need a summary document to brief another agent. The entire category of "work about work" exists because humans are bandwidth-constrained. AI isn't.

So when Salesforce starts charging per task and enterprises start tracking cost per Agentic Work Unit, the result is to stop doing the tasks that only existed because humans were running the company. Which means that many humans will no longer be able to run the company. (Or even needed?)

I think one truth that a lot of people don't realize yet is that most of what we call "work" is actually coordination overhead. Charging per task just exposes organizational waste that was previously invisible because human labor made it feel free.

That's the other narrative that's wrong about the SaaSpocalypse. It not about AI replacing software, it's about AI revealing how much of what software automated was never worth doing in the first place.

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7430266601754734593

What will knowledge work be in 18 months? Look at what AI is doing to coding right now for the roadmap.

Today, AI is "farther" down the path of changing how coders operate versus knowledge workers. But it's the same path. In fact you can take almost anything written about how AI is impacting coders, replace a few code-specific terms with the equivalent knowledge work terms, and BAM! You instantly have a strategy guide laying out exactly what's going to happen to knowledge work in the next 12-18 months. üîÆ

I explore this in detail in this week's Citrix blog post: https://lnkd.in/gc-vna44 (Also this will probably save you a couple hundred thousand dollars in McKinsey consulting fees.)

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7429929349899915264

Anthropic CEO Dario Amodei just wrote his most important AI essay since "Machines of Loving Grace" in 2024. It's long, but you should read it anyway.

Most people are focusing on his prediction about AI cutting half of entry level jobs in 1-5 years. But there are some other really important points in his essay that I want to call out:

First, Amodei asks that as AI becomes more capable at knowledge work, do you use it to cut costs (replace workers) or to innovate (expand what's possible)? Both are rational, but obviously produce completely different companies. You can't have both. Which side are you on? Which side is your company on? Are you aligned?

The second new thing for me is his talk about the "diffusion gap", which is the time between "AI can do this" and "our institutions have adjusted." Previous technology waves took decades to diffuse. (e.g. web apps could replace desktop apps in the 1990s, but it took until the 2020s for that to be pretty fully done.)

I don't know how fast AI capabilities will diffuse, but if some companies are slow and some are fast, the compounding nature of AI will probably create some pretty wild imbalances.

I write a lot about how enterprises have natural limitations to how fast they can adopt transformational new technologies (compliance, existing systems, sunk costs, decades of routines, etc.) That's essentially what causes the diffusion delay. But also, if very powerful AIs exist that could actually do all the powerful things a company needs, will a company that can diffuse AI more quickly (e.g. using it for innovation) end up killing off the cost cutting companies?

There are so many flashing signals now (just in the past few months) that we are turning a corner with AI impacting knowledge work. If true, then what Amodei writes in this essay will be extremely relevant to all of us in the very near future.

Take an hour to read this, especially if you're in any type of leadership position: https://lnkd.in/eqNCN_Xc

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7429505710536896512

Hey creators & experts! I will pay to subscribe to your brain.

I don't mean your newsletter, podcast, or course. I mean your personal AI knowledge system (what I call the "second brain") which includes your actual evolving structured thinking, plugged directly into my own second brain so your perspectives compound alongside mine every day.

Let's face it: you're scrambling to stay in front of the AI slop machine that's eating your livelihood, and your followers and subscribers are already scraping everything you publish and integrating it into their own AI knowledge systems anyway.

So let's both skate to where the puck is going. I respect you and want to pay you for your work. You want to get back to actually thinking, pontificating, and ideating. I want to integrate your perspectives into my second brain. You want to get out of the "competing with AI" rat race.

Let's make a deal!

A subscribable brain sits somewhere between a newsletter and an analyst subscription in terms of value, but the delivery mechanism is fundamentally better. I already built a living knowledge system that my AI reasons over which gets better every time I refine my thinking and use it. Now I want to properly integrate the thinking of the experts I follow.

So Nate B. Jones, Paul Roetzer, Ethan Mollick, Simon Willison, Daniel Miessler, Aaron Levie, Dwarkesh Podcast, Christopher Penn, and Dharmesh Shah, I'm ready to start paying my subscription fees and syncing your public GitHub knowledge repo into mine. Let's go!

I dig into all the details here:

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7428076319583657984

‚ÄúAI will never be able to capture my voice from a one-sentence prompt!‚Äù True! (But misleading!)

This scenario should never exist. Sure, I may prompt my AI with a single sentence, and it may produce amazing results. But that‚Äôs because the ‚Äúreal‚Äù prompt was the past 10,000 sentences I wrote, dictated, pontificated , and shaped.

AI working from ‚Äújust a single sentence‚Äù will not be a thing in the future.

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7427600067403890688

I just had my ‚ÄúMove 37‚Äù moment for AI & knowledge work. Something big is happening in AI, right now. Like, RIGHT NOW. You can read what I wrote here and on the Citrix blog about the ‚Äúsecond brain‚Äù concept, and how it‚Äôs fundamentally changing knowledge work. Don‚Äôt dwell on the second brain name. Call it ‚Äúvery powerful and always updating AI that knows your full context always.‚Äù Maybe it‚Äôs Claude Cowork. Doesn‚Äôt matter exactly, the important thing is something is happening in AI now. Great take from Matt Shumer if you want perspectives other than mine:

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7427098660292694016

My colleague Dave Brear is the one who introduced me to the concept of a personal AI knowledge system or "second brain." He demoed his system to me on an hour long call. An hour later, I had my own version up and running. Two hours later I knew work would never be the same again...

Check out Dave's brilliant take on this moment (which he calls the "stage 1 jettisoned, stage 2 firing" moment). What's happening in this moment of knowledge worker AI is real, and you don't have to take MY word for it:

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7426969452602208256

This might be the important thing I've written in 30+ years of enterprise IT.

I built a second brain‚Äîa folder of plain text files on my laptop that an AI reads, maintains, and builds on every day. It has my frameworks, my positions, my in-progress thinking, everything I've figured out over three decades. And it compounds daily.

I knew within two hours I'd never go back.

But that's not the story. Once I realized that this allows AI to integrate with the 80% of my brain that isn't visible from the outside (e.g. the "real" thinking, strategizing, judging, versus the emails, docs, and scheduling), I realized that when scaled, this is going to fundamentally change knowledge work.

What's most wild is I'm doing this now. Like, I have changed how I work and living this future. I think a few hundred thousand people are too. It's about to be millions, and holy moly, companies are not ready.

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7425262298182619138

I want to push back on the "AI makes us dumber" trope. This can be true, but doesn't have to be.

Of course, any time you stop doing something, you can lose that skill. So if you use AI to "offload your thinking", you will "think less" and possibly lose that skill.

But in my case, I use AI a lot. But not to "think for me", rather to handle all the structural busywork. (formatting, organizing, scaffolding...) So in my case, AI frees me to do MORE actual thinking, not less.

Thanks to AI, I now spend more of my day in genuine cognitive work now than I did before. The tedious stuff that used to consume mental cycles is gone. What's left is the hard thinking.

The "AI makes us dumber" is an shallow platitude. How you use AI determines the outcome. Delegate the mechanical, keep the intellectual!

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7424819222431825921

OpenClaw is blowing up this week, and yes, it's a security nightmare. Block it. But that's not the real story!

The story is why hundreds of thousands of people are using it anyway. (despite knowing the risks!) They're not being reckless. They're showing us what they actually want from AI: something that works with them personally, not a chatbot that answers questions.

The gap between "what workers want" and "what companies provide" is the real governance problem. And it's not going away when OpenClaw does.

I wrote more about what this means for corporate AI strategy on the blog today: https://lnkd.in/esnsnMNs

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7424424064208867329

I feel like AI models are like CPUs: I don't really care which one I'm using, as the overall experience is more about everything else surrounding it.

What's actually important? The context, what data sources it has access to, how big its context window and reasoning budgets are, what connectors are active, etc... But when all that's the same, it doesn't make a super huge difference whether I'm using GPT 5.3 or Opus 4.5 or Gemini 3.

Sure, the relative generation & size of the model is important. Just like an Apple M4 is much more advanced than an M1. But when comparing the generally-available latest models from each lab, I honestly don't really care which one I'm using. (The model is no longer the moat, as my AI would say.)

But, "we have the best AI system because we have the best models" doesn't seem like it's going to be a thing.

Feb 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7422265880903946240

Ethan Mollick's observation about Claude in Excel is the bitter lesson playing out in real-time. Microsoft built an Excel agent that uses Excel "properly" -- VLOOKUPs, native functions, respecting the application's architecture, etc. Anthropic built one that ignores all that, runs Python, and treats Excel as just a source of input and output. ü§£ 

Anthropic's approach wins. Handily.

This is Rich Sutton's bitter lesson from AI research: simple methods that leverage raw computation always beat sophisticated human-engineered approaches. Microsoft tried to be clever by working with the app. Anthropic said "Who cares about the app?" and just solved the problem.

The app became middleware the AI didn't need.

Of course if AI doesn't care about your apps, your governance model can't be app-centric either. The workspace (where identity, security, and context converge) is the only durable control plane when the apps themselves become optional.

Ethan asks who wins the agentic workspace. My answer is whomever governs it.

Jan 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7421882712841474048

Wow! Claude can now render connected app UIs directly inside the conversation. (Slack, Figma, Asana, Box, etc.) This is not just connecting in the background, as now you see the actual interface, edit content, & interact with the app without switching tabs.

This is what the "AI as the interface" that I wrote about last month looks like.

I'm telling you: AI platforms are going to be the primary way workers interact with their apps & data. Everything else will recede into infrastructure that AI operates on your behalf.

For enterprise IT, this raises familiar questions. What data flows where when Claude is operating your enterprise apps? What audit trails exist? How does this integrate with existing DLP & access controls? Who decides which employees can connect which AIs to which apps?

This should feel familiar to those of us who've been in the Citrix/EUC universe for decades. Now we're watching AI platforms pull app interfaces into the AI conversations as the new workspace layer. So, slightly different tech, but all the same governance issues: secure the work, regardless of where the UI renders.

I imagine the next step will be the AI platform "reimagining" the interface. Maybe it just pulls out and highlights the most relevant parts? Maybe it acts as a voice interface to the old app? Maybe it pulls out the most important data and projects it into your AR glasses?

BTW even if Claude is the first to due this, it's going to soon be everywhere. The companies who figure out how to enable this safely will have a head start.

Jan 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7420037054555746304

Most customers I talk to are still thinking about the risks of employees using AI like a DLP problem. (e.g. worker leaks corp. secrets into some random model.)

But the real risk moving forward will be from agents performing actions using the worker's permissions, where the worker doesn't 100% pay attention to what the AI's doing, and the AI leaks the secrets (or performs some other negative action) on its own.

This is what I dig into in this week's post on the Citrix blog: https://lnkd.in/e2WVGh8f

Jan 2026

---

https://www.linkedin.com/feed/update/urn:li:share:7416878361073053698

Most of knowledge workers' "work" is invisible to outsiders. After all, they can only see outputs: docs created, meetings attended, emails, etc. This is why corporate-led AI transformations fail. They can't see, index, or build AI for the 80% that lives in workers' heads.

This is why worker-led AI actually transforms work: workers know their own 100% and bring it to AI in a thousand little ways every day.

More in this week's post: https://lnkd.in/e8w2m5rh

Jan 2026

---

https://www.linkedin.com/feed/update/urn:li:ugcPost:7416526353098825728

Here's everything I learned about how AI will affect the workplace in 2025, in one place:

23 blog posts, 4 keynotes, and a bunch of podcasts, organized by theme so you can find what's useful: the post-application era, worker-led AI as shadow strategy, agents as insider threats, why boring infrastructure beats moonshots, and how much fun I'm having at Citrix. ü•∞

Jan 2026

