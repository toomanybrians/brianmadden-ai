---
title: "How much \"help\" did AI provide? We should all disclose this."
date: "2023-06-26"
authority_level: 5
file_type: linkedin-article
tags: [ai-disclosure, ai-influence-levels, ai-transparency, copilot, workplace-ai]
related_frameworks: []
original_url: "https://www.linkedin.com/pulse/how-much-help-did-ai-provide-we-should-all-disclose-brian-madden/"
staleness_threshold: stable
---

# How much "help" did AI provide? We should all disclose this.

*Brian Maddenâ€”June 2023*
*Published: https://www.linkedin.com/pulse/how-much-help-did-ai-provide-we-should-all-disclose-brian-madden/*

*Header image: Collage of AIL level badges 0-5.*

With so many people using ChatGPT now, it's only a matter of time before we're flooded with so much AI-generated content that we won't even know what's actually written by a human. So the concept of an "AI influence disclosure" is something we should think about now.

People have been talking about something like this for media content. (e.g. Has this image/song/video been generated or altered by AI?) But it's not something that comes up much for text, which is going to be just as important (especially in the business world).

I learned about it via Daniel Miessler, one of my favorite AI thought leaders. In a post called *AI Influence Level (AIL) v1.0*, Miessler proposes a 0-5 disclosure scale which could be attached to a piece of content:

*Image: AIL v1.0 scale from Daniel Miessler (danielmiessler.com), showing levels 0 through 5 of AI influence on content creation.*

His idea is that you would then say a piece of content is "AIL 1" or "AIL 4". I like this a lot. It seems like a good enough starting point and I'm sure it will evolve over time. (For instance, I notice that Miessler's scale doesn't use the color red, so we'll easily be able to add AIL6 in the future: AI Created, Humans Unable to Intervene.)

## AIL disclosures help prevent AI model collapse

A bonus benefit of AIL disclosures is they can help future AI models while they're training. Obviously the internet is already full of a lot of junk which would be problematic for model training. Model training data quality is a complicated issue and will be the topic of many articles in the future, but AI model builders are already careful about what they feed into their models. (Contrary to popular belief, the big models don't just "crawl the whole internet" because that would be gross and plus training models is slow and expensive and you don't want to waste those resources.)

But in addition to training models on high-quality data, it's also important to train them on human-created data, so if more and more content on the internet is generated by AI, how does the model know what's good human stuff and what's AI-created noise?

There's actually a fascinating concept called AI Model Collapse, which is what happens once so many people use generative AI to create content that future AI models accidentally train on AI-created content instead of primarily human-created content. It's like a copy of a copy of a copy except it goes way crazier.

So, while there will be several ways to address this, knowing how much influence an AI had over a particular piece of content is useful for both humans and AIs.

## Today this is "just blogs". Tomorrow it will be every written word from work.

Have you seen the demo for Microsoft 365 Copilot? If not, watch this 90-second video.

Microsoft 365 Copilot is only available as a preview for certain customers now, but most peoples' reactions when they see this video is, "OMG TAKE MY MONEY NOW!" Google announced a similar roadmap for Google Workspaces customers.

As great as this is at first blush, after letting it sink in, you start to wonder whether giving your coworkers powerful tools which make it easier for them to send more and longer emails is actually a good thing?

So I want an AIL disclosure for these as well. (Not just emails, but documents, memos, presentations... everything! Maybe it's a page-by-page disclosure.)

Ideally it would be great to be able to setup inbox filters, or search filters based on different levels.

It's important that the AI influence is a sliding scale (like the 0-5), and not just "Did AI help, yes or no?" since sometimes you might care about the difference between something a human created and then AI helped clean up, versus something that AI conceived and built completely on its own.

Do I want to read everything that an AI built at work? Probably not. Emails and general day-to-day stuff, meh, humans only please! But if I'm doing research on a topic and I want to know everything I can, maybe AI-generated content is ok.

Sidebar: Having Copilot write emails and create content for others is only the tip of the iceberg in terms of how AI can help in the office. The real power will be using a ChatGPT-like interface to ask questions about products and strategy, to have the AI superbrain scan all internal work docs and my whole inbox and tell me what to do next. Hopefully in that world, these productivity theater acts of AI writing emails and meeting summaries will be a short lived v1 attempt at integrating AI into the office.

Microsoft also showed off Copilot capabilities which summarize a person's inbox or todo lists, a task which will require AI to address the deluge of incoming written text from all the AI summaries. You think the cat-and-mouse game of email spamming and filtering is bad, imagine if it's internal, the company is paying for it, and everyone wants to generate as much as they can since that's historically how people have demonstrated value. (Meanwhile the AI vendors are getting paid on both sides of this. Bravo!)

But having AIL disclosures, whether attached to documents, or embedded in metadata, will be a big help for workers.

## Did the sender even read this?

That professor I mentioned last week talked about spending 45 minutes grading papers where the students used ChatGPT. For me, the takeaway from that was not him wasting all that time grading a paper the student didn't write, but rather whether the student even read it!

Is that a disclosure that's possible? So when I get that email from my boss, sure, I want to know how much AI influenced them, but I also want to know whether they even read the final result they sent out. (Or, heck, do they even know an email was sent out on their behalf?)

## Will this flow into personal relationships too?

The question of AI influence and disclosure doesn't end at the office, as these same issues could arise in personal relationships too. How tempting is it to ask ChatGPT to write a perfect poem for your loved one. Do you tell them? Is this a conversation you have after the fact? Or now?

Remember, the level of AI influence is a sliding scale of influence, not a binary "AI was used" or not. At one level, this is no different than a greeting card. (I have the feelings but don't know how to express them.) The difference is that everyone knows greeting cards were written by someone else, but if you write a love limerick, will your relationship require you to disclose this?

I could imagine a whole new line of relationship disclosures: "I'm comfortable with any AIL as long as its disclosed. For major birthdays I want AIL2 or lower. For family vacations, AIL4 or lower is ok."

It's a conversation we'll all start having in all areas of our lives. The key is not that AI is bad, or that using AI is bad, rather than AI's influence should be disclosed to the consumer of the content.

## How will these disclosures work?

Even if we agree this is a good concept, now what? I like the idea of using the AIL 0-5 scale as a start, and I like the idea of tagging content with it.

I've seen some people just use text disclosures, like Text: AIL0, Graphics: AIL4. Some people use emoji.

I think for me, I'm going to just put an image at the end of each article. (And maybe in my email footer and slides?) I'm not sure yet, so we'll see where this goes.

*This text is AI Influence Level 0: Human conceived & created.*
