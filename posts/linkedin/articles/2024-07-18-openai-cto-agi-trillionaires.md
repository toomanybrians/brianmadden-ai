---
title: "Open AI CTO reminds us they care more about inventing AGI and becoming trillionaires than your company's safety"
date: "2024-07-18"
authority_level: 5
file_type: linkedin-article
tags: [openai, workplace-ai, enterprise-risk, ai-governance, shadow-it, ai-policy]
related_frameworks: []
original_url: "https://www.linkedin.com/pulse/open-ai-cto-reminds-us-care-more-inventing-agi-becoming-brian-madden-42l6c/"
staleness_threshold: stable
---

# Open AI CTO reminds us they care more about inventing AGI and becoming trillionaires than your company's safety

*Brian Madden—July 2024*
*Published: https://www.linkedin.com/pulse/open-ai-cto-reminds-us-care-more-inventing-agi-becoming-brian-madden-42l6c/*

*Header image: Slide from Brian's talk about Workplace AI.*

My entire focus on Artificial Intelligence is through the lens of how it will impact the traditional corporate workplace. One thing I've noticed is many of the new hot AI vendors are quite different than the traditional software vendors companies have historically relied on. While this can be cool in the "be agile" and "shake up the status quo" way, it's scary for those of us who are responsible for boring worky things like compliance and risk management.

This point was perfectly illustrated by Open AI CTO Mira Murati (YouTube link), who recently said:

> Inside the labs we have these capable models, and they're not that far ahead from what the public has access to for free. And that's a completely different trajectory for bringing technology into the world that what we've seen historically. It's a great opportunity because it brings people along. It gives them intuitive sense for the capabilities and risks and allows people to prepare for the advent of bringing advanced AI into the world.

My initial reaction to this was, "Wow, yeah! How cool is it that these companies are so unlike the traditional companies who provide software for enterprises? This is the future!!!"

It's easy to understand why Open AI takes this approach:

- These foundational models cost hundreds of millions of dollars to train and tune
- The competition for the "best" model is fierce
- The shelf-life of the latest model is limited

Open AI can't afford to have GPT.next fully cooked yet sitting on the shelf, costing them billions of dollars of lost market value, while the boring business nerds try to figure out the risk. Once this thing is built, they need to get it out into the market ASAP!

Again, I understand this. But that doesn't mean I like it.

Thinking about this from the corporate customer standpoint—yikes! Yes, Murati is right, this is completely different than what we've seen historically. That's what's so scary about it. Open AI's "great opportunity to bring people along" which gives their users "the intuitive sense for the capabilities and risks" means that users and customers are figuring out what the risks are, rather than Open AI.

She's absolutely right—this is not how enterprise software is traditionally created or sold.

## The risk is your employees don't care

The problem for companies is that Open AI's software is available directly to employees, regardless of whether the company knows about it or even allows it. Employees have access to powerful technologies that they're using to replace parts of their jobs (even if it's "just" for brainstorming or drafting emails), and not only do the employees not understand the capabilities and risks, but the people building these tools are not that far ahead of them in understanding those capabilities and risks either.

This is scary, and why it's critical for all companies to start to understand what AI capabilities are available to their employees today, and to figure out what they should support and what they should discourage.

As we've seen again and again, taken as a whole, employees are going to use the tools that they perceive will help them, and this is especially true in areas where companies haven't created and communicated real-reasoned policies to their employees.

## What should you do about it?

The ways that companies recognize and manage this risk needs to evolve. It's not as simple as "banning ChatGPT" or simply ignoring the problem and letting individual employees figure things out on their own.

That said, it seems like we're all sort of figuring this out together, which is one of the core reasons I launched the Workplace AI Strategy Guide project which I wrote about last week.

One thing we know for certain is that even if the AI hype bubble pops, technologies like ChatGPT are not going away, and more and more employees will continue to discover and use them. As a business, we need to understand this and engage with employees to figure out how these technologies can be used and what the risks are.

What's your strategy for handling this? Cross-functional AI governance teams? AI literacy for employees? Creating AI acceptable use policies? Ignoring it? Let's discuss!

*The Workplace AI Strategy Guide project is at strategyguide.ai.*

*This article is AI Influence Level 0: Human created and conceived with no AI assistance. Details at strategyguide.ai/ail.*
